{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1b3f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    f1_score, precision_score, recall_score, hamming_loss,\n",
    "    label_ranking_average_precision_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from lightgbm import LGBMClassifier, early_stopping\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e8ea4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Load & Clean Data ------------------\n",
    "df = pd.read_csv(\"Data_Final.csv\")\n",
    "df['TotalCount'] = pd.to_numeric(df['TotalCount'], errors='coerce')\n",
    "df = df.dropna(subset=['TotalCount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e1c940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Filter Based on TotalCount ------------------\n",
    "group_total = df.groupby(\"MasterTestCode\")[\"TotalCount\"].first()\n",
    "threshold = group_total.quantile(0.2)  # Customize as needed\n",
    "print(f\"\\nðŸŽ¯ Ø¢Ø³ØªØ§Ù†Ù‡ ÙÛŒÙ„ØªØ±: {threshold:.0f}\")\n",
    "\n",
    "valid_codes = group_total[group_total >= threshold].index\n",
    "df = df[df[\"MasterTestCode\"].isin(valid_codes)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae7c4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Filter ------------------\n",
    "# ÙÛŒÙ„ØªØ± Ø¨Ø§ Ø¢Ø³ØªØ§Ù†Ù‡ Ù‚Ø§Ø¨Ù„ ØªÙ†Ø¸ÛŒÙ…\n",
    "# (Ù…Ø«Ù„Ø§Ù‹ ØµØ¯Ú© 20Ùª Ø¨Ø±Ø§ÛŒ Ø­Ø°Ù Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ú©Ù…â€ŒÙ†Ù…ÙˆÙ†Ù‡)\n",
    "threshold = group_total.quantile(0.2)\n",
    "print(f\"\\nðŸŽ¯ Ø¢Ø³ØªØ§Ù†Ù‡ ÙÛŒÙ„ØªØ±: {threshold:.0f}\")\n",
    "\n",
    "valid_codes = group_total[group_total >= threshold].index\n",
    "df_filtered = df[df[\"MasterTestCode\"].isin(valid_codes)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8023cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Feature Engineering ------------------\n",
    "for col in [\"FeBase\", \"Destruct\", \"IsLarge\"]:\n",
    "    df[col] = df[col].fillna(0).astype(int)\n",
    "\n",
    "# Load dependency info\n",
    "strong_deps = pd.read_csv(\"strong_test_dependencies.csv\")\n",
    "strong_counts = strong_deps.groupby(\"Test1\").size().reset_index(name=\"StrongDepCount\")\n",
    "df = df.merge(strong_counts, left_on=\"MasterTestCode\", right_on=\"Test1\", how=\"left\")\n",
    "df[\"StrongDepCount\"] = df[\"StrongDepCount\"].fillna(0)\n",
    "\n",
    "# Mean physical attributes by MasterTestCode\n",
    "avg_phys = df.groupby(\"MasterTestCode\")[[\"FeBase\", \"Destruct\", \"IsLarge\"]].mean().reset_index()\n",
    "df = df.merge(avg_phys, on=\"MasterTestCode\", suffixes=(\"\", \"_mean\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aecbaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-based Features\n",
    "df[\"MaxDate\"] = pd.to_datetime(df[\"MaxDate\"], errors=\"coerce\")\n",
    "now = pd.Timestamp.now()\n",
    "df[\"TestAgeDays\"] = (now - df[\"MaxDate\"]).dt.days.clip(lower=1)\n",
    "df[\"TimeWeight\"] = 1 / df[\"TestAgeDays\"]\n",
    "\n",
    "# Count-based Features\n",
    "df[\"TestImportance\"] = df[\"TestCount\"] * df[\"TimeWeight\"]\n",
    "df[\"LogTotalCount\"] = np.log1p(df[\"TotalCount\"])\n",
    "\n",
    "# Weighted Features\n",
    "df[\"WF_FeBase\"] = df[\"FeBase\"] * df[\"TestImportance\"]\n",
    "df[\"WF_Destruct\"] = df[\"Destruct\"] * df[\"TestImportance\"]\n",
    "df[\"WF_IsLarge\"] = df[\"IsLarge\"] * df[\"TestImportance\"]\n",
    "df[\"WF_StrongDep\"] = df[\"StrongDepCount\"] * df[\"TestImportance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728347f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping\n",
    "sample_features = df.groupby(\"SampleName\").agg({\n",
    "    \"MasterTestCode\": lambda x: list(x.dropna()),\n",
    "    \"WF_FeBase\": \"mean\",\n",
    "    \"WF_Destruct\": \"mean\",\n",
    "    \"WF_IsLarge\": \"mean\",\n",
    "    \"WF_StrongDep\": \"mean\",\n",
    "    \"FeBase_mean\": \"mean\",\n",
    "    \"Destruct_mean\": \"mean\",\n",
    "    \"IsLarge_mean\": \"mean\",\n",
    "    \"LogTotalCount\": \"mean\"\n",
    "}).rename(columns={\n",
    "    \"WF_FeBase\": \"FeBase_weighted\",\n",
    "    \"WF_Destruct\": \"Destruct_weighted\",\n",
    "    \"WF_IsLarge\": \"IsLarge_weighted\",\n",
    "    \"WF_StrongDep\": \"StrongDepCount_weighted\"\n",
    "}).reset_index()\n",
    "\n",
    "# Ø­Ø°Ù Ø³Ø·Ø±Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ù„ÛŒØ³Øª Ø¨Ø±Ú†Ø³Ø¨Ø´ÙˆÙ† Ø®Ø§Ù„ÛŒÙ‡\n",
    "sample_features = sample_features[sample_features[\"MasterTestCode\"].apply(lambda x: len(x) > 0)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272900da",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_features[\"SampleName\"] = sample_features[\"SampleName\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1c59ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Ø³Ø§Ø®Øª TF-IDF\n",
    "sample_vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1, 2), max_features=20000)\n",
    "\n",
    "X_text = sample_vectorizer.fit_transform(sample_features[\"SampleName\"].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e119ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ\n",
    "X_numeric = sample_features.drop(columns=[\"MasterTestCode\", \"SampleName\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b995a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. ØªØ¨Ø¯ÛŒÙ„ Ø¹Ø¯Ø¯ÛŒâ€ŒÙ‡Ø§ Ø¨Ù‡ sparse\n",
    "X_numeric_sparse = csr_matrix(X_numeric.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbe7b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. ØªØ±Ú©ÛŒØ¨ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§\n",
    "X_final = hstack([X_numeric_sparse, X_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870e0262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Binarize Labels ------------------\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(sample_features[\"MasterTestCode\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e158d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Train/Test Split\n",
    "sample_features[\"MasterTestCode\"] = sample_features[\"MasterTestCode\"].apply(\n",
    "    lambda x: x if isinstance(x, list) else []\n",
    ")\n",
    "\n",
    "# Ø³Ø§Ø®Øª encoder Ø¨Ø±Ø§ÛŒ Ú†Ù†Ø¯Ø¨Ø±Ú†Ø³Ø¨ÛŒ\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "# Ø¨Ø§ÛŒÙ†Ø±ÛŒâ€ŒØ³Ø§Ø²ÛŒ Ù„ÛŒØ¨Ù„â€ŒÙ‡Ø§\n",
    "y = mlb.fit_transform(sample_features[\"MasterTestCode\"])\n",
    "\n",
    "# ------------------ Train/Test Split ------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_final, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b634133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ min_data_in_leaf ------------------\n",
    "label_pos_counts = y.sum(axis=0)\n",
    "valid_mask = label_pos_counts > 0\n",
    "avg_positives_per_class = np.mean(label_pos_counts[valid_mask])\n",
    "min_data_leaf_value = int(max(50, min(500, avg_positives_per_class * 0.2)))\n",
    "print(f\"ðŸ”§ Ù…Ù‚Ø¯Ø§Ø± ØªÙ†Ø¸ÛŒÙ…â€ŒØ´Ø¯Ù‡ min_data_in_leaf: {min_data_leaf_value}\")\n",
    "\n",
    "# ------------------ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ú©Ù„Ø§Ø³ ------------------\n",
    "n_classes = y.shape[1]\n",
    "\n",
    "def train_model_for_class(i):\n",
    "    y_i = y[:, i]\n",
    "    if y_i.sum() < 5:\n",
    "        return None\n",
    "\n",
    "    X_train, X_val, y_train_i, y_val_i = train_test_split(\n",
    "        X_final, y_i, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    model = LGBMClassifier(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.05,\n",
    "        random_state=42,\n",
    "        n_jobs=2,\n",
    "        is_unbalance=True,\n",
    "        min_data_in_leaf=min_data_leaf_value\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train_i,\n",
    "        eval_set=[(X_val, y_val_i)],\n",
    "        eval_metric=\"binary_logloss\",\n",
    "        callbacks=[early_stopping(stopping_rounds=20)],\n",
    "        verbose=-1\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# ------------------ Ø¢Ù…ÙˆØ²Ø´ Ù…ÙˆØ§Ø²ÛŒ ------------------\n",
    "print(\"ðŸš€ Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´ Ù…ÙˆØ§Ø²ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§...\")\n",
    "models = Parallel(n_jobs=2)(  # Ø¨Ø³ØªÙ‡ Ø¨Ù‡ RAM Ùˆ CPU Ù‚Ø§Ø¨Ù„ Ø§ÙØ²Ø§ÛŒØ´Ù‡\n",
    "    delayed(train_model_for_class)(i) for i in tqdm(range(n_classes))\n",
    ")\n",
    "\n",
    "# ------------------ ÙÛŒÙ„ØªØ± Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ù…Ø¯Ù„ Ù†Ø¯Ø§Ø±Ù†Ø¯ ------------------\n",
    "models = [m for m in models if m is not None]\n",
    "\n",
    "# ------------------ Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ Ùˆ Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ ------------------\n",
    "os.makedirs(\"model_output_new\", exist_ok=True)\n",
    "joblib.dump(models, \"model_output_new/lightgbm_models.pkl\")\n",
    "joblib.dump(sample_vectorizer, \"model_output_new/vectorizer.pkl\")\n",
    "joblib.dump(mlb, \"model_output_new/label_binarizer.pkl\")\n",
    "\n",
    "print(\"âœ… Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ddf1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training\n",
    "#model = LogisticRegression(max_iter=1000)\n",
    "#model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6072f187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "micro_f1 = f1_score(y_test, y_pred, average=\"micro\")\n",
    "micro_precision = precision_score(y_test, y_pred, average=\"micro\")\n",
    "micro_recall = recall_score(y_test, y_pred, average=\"micro\")\n",
    "lrap = label_ranking_average_precision_score(y_test, model.predict_proba(X_test))\n",
    "hamming = hamming_loss(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6e63e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nâœ… Ù†ØªØ§ÛŒØ¬ Ù…Ø¯Ù„:\")\n",
    "print(f\"Micro F1: {micro_f1:.4f}\")\n",
    "print(f\"Micro Precision: {micro_precision:.4f}\")\n",
    "print(f\"Micro Recall: {micro_recall:.4f}\")\n",
    "print(f\"LRAP: {lrap:.4f}\")\n",
    "print(f\"Hamming Loss: {hamming:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb0f7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Histogram\n",
    "plt.figure(figsize=(14, 6))\n",
    "df.groupby(\"MasterTestCode\")[\"TotalCount\"].first().hist(bins=30, log=True)\n",
    "plt.xlabel(\"TotalCount\")\n",
    "plt.ylabel(\"ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ (log)\")\n",
    "plt.title(\"Ù¾Ø±Ø§Ú©Ù†Ø¯Ú¯ÛŒ TotalCount Ø¨Ø± Ø§Ø³Ø§Ø³ MasterTestCode (Ø¨Ø¹Ø¯ Ø§Ø² ÙÛŒÙ„ØªØ±)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (RasfAi)",
   "language": "python",
   "name": "new_rasfai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
