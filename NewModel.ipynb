{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b510e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from lightgbm import LGBMClassifier\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de907c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Load Base Data ------------------\n",
    "df = pd.read_csv(\"Data_Final.csv\")\n",
    "df.columns = df.columns.str.strip()\n",
    "df = df.dropna(subset=[\"SampleName\", \"MasterTestCode\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89392cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Feature Engineering: Final with Time & Count Weights + SampleName Text ------------------\n",
    "# Ensure binary columns\n",
    "for col in [\"FeBase\", \"Destruct\", \"IsLarge\"]:\n",
    "    df[col] = df[col].fillna(0).astype(int)\n",
    "\n",
    "# Load dependency info\n",
    "strong_deps = pd.read_csv(\"strong_test_dependencies.csv\")\n",
    "strong_counts = strong_deps.groupby(\"Test1\").size().reset_index(name=\"StrongDepCount\")\n",
    "df = df.merge(strong_counts, left_on=\"MasterTestCode\", right_on=\"Test1\", how=\"left\")\n",
    "df[\"StrongDepCount\"] = df[\"StrongDepCount\"].fillna(0)\n",
    "\n",
    "# Mean physical attributes by MasterTestCode\n",
    "avg_phys = df.groupby(\"MasterTestCode\")[[\"FeBase\", \"Destruct\", \"IsLarge\"]].mean().reset_index()\n",
    "df = df.merge(avg_phys, on=\"MasterTestCode\", suffixes=(\"\", \"_mean\"))\n",
    "\n",
    "# --- Time-based Features ---\n",
    "df[\"MaxDate\"] = pd.to_datetime(df[\"MaxDate\"], errors=\"coerce\")\n",
    "now = pd.Timestamp.now()\n",
    "df[\"TestAgeDays\"] = (now - df[\"MaxDate\"]).dt.days.clip(lower=1)\n",
    "df[\"TimeWeight\"] = 1 / df[\"TestAgeDays\"]\n",
    "\n",
    "# --- Count-based Features ---\n",
    "df[\"TestImportance\"] = df[\"TestCount\"] * df[\"TimeWeight\"]\n",
    "df[\"LogTotalCount\"] = np.log1p(df[\"TotalCount\"])\n",
    "\n",
    "# --- Weighted Feature Columns ---\n",
    "df[\"WF_FeBase\"] = df[\"FeBase\"] * df[\"TestImportance\"]\n",
    "df[\"WF_Destruct\"] = df[\"Destruct\"] * df[\"TestImportance\"]\n",
    "df[\"WF_IsLarge\"] = df[\"IsLarge\"] * df[\"TestImportance\"]\n",
    "df[\"WF_StrongDep\"] = df[\"StrongDepCount\"] * df[\"TestImportance\"]\n",
    "\n",
    "# --- Group by SampleName ---\n",
    "sample_features = df.groupby(\"SampleName\").agg({\n",
    "    \"MasterTestCode\": lambda x: \" \".join(str(i) for i in x.dropna()),\n",
    "    \"WF_FeBase\": \"mean\",\n",
    "    \"WF_Destruct\": \"mean\",\n",
    "    \"WF_IsLarge\": \"mean\",\n",
    "    \"WF_StrongDep\": \"mean\",\n",
    "    \"FeBase_mean\": \"mean\",\n",
    "    \"Destruct_mean\": \"mean\",\n",
    "    \"IsLarge_mean\": \"mean\",\n",
    "    \"LogTotalCount\": \"mean\"\n",
    "})\n",
    "\n",
    "sample_features.rename(columns={\n",
    "    \"WF_FeBase\": \"FeBase_weighted\",\n",
    "    \"WF_Destruct\": \"Destruct_weighted\",\n",
    "    \"WF_IsLarge\": \"IsLarge_weighted\",\n",
    "    \"WF_StrongDep\": \"StrongDepCount_weighted\"\n",
    "}, inplace=True)\n",
    "\n",
    "# --- TF-IDF of SampleName (for similarity like Ù¾ÛŒÚ† m24 vs m42) ---\n",
    "sample_features[\"SampleName\"] = sample_features.index.astype(str)\n",
    "sample_vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1, 2))\n",
    "X_sample_name_text = sample_vectorizer.fit_transform(sample_features[\"SampleName\"])\n",
    "\n",
    "print(\"âœ… ÙÛŒÚ†Ø±Ù‡Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø§ Ø²Ù…Ø§Ù†ØŒ ØªÚ©Ø±Ø§Ø± Ùˆ Ù…ØªÙ† SampleName Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯Ù†Ø¯.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6deb15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Vectorize SampleName ------------------\n",
    "vectorizer = TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(3, 5), max_features=1000)\n",
    "X_text = vectorizer.fit_transform(df[\"SampleName\"].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb11f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "# 1. TF-IDF Ø§Ø² MasterTestCode (ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ ØªØ³Øªâ€ŒÙ‡Ø§)\n",
    "X_text = vectorizer.transform(sample_features[\"MasterTestCode\"])\n",
    "\n",
    "# 2. ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ù†Ù‡Ø§ÛŒÛŒ\n",
    "X_numeric = sample_features[[\n",
    "    \"FeBase_weighted\", \"Destruct_weighted\", \"IsLarge_weighted\",\n",
    "    \"StrongDepCount_weighted\",\n",
    "    \"FeBase_mean\", \"Destruct_mean\", \"IsLarge_mean\",\n",
    "    \"LogTotalCount\"\n",
    "]].fillna(0).values\n",
    "X_numeric_sparse = csr_matrix(X_numeric)\n",
    "\n",
    "# 3. ÙˆÛŒÚ˜Ú¯ÛŒ Ù…ØªÙ†ÛŒ SampleName (TF-IDF)\n",
    "# (ÙØ±Ø¶ Ø¨Ø± Ø§ÛŒÙ†Ú©Ù‡ sample_vectorizer Ùˆ X_sample_name_text Ø§Ø² Ø³Ù„ Ù‚Ø¨Ù„ÛŒ Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯Ù†)\n",
    "\n",
    "# 4. ØªØ±Ú©ÛŒØ¨ Ù†Ù‡Ø§ÛŒÛŒ Ù‡Ù…Ù‡ ÙÛŒÚ†Ø±Ù‡Ø§\n",
    "X_all = hstack([X_sample_name_text, X_text, X_numeric_sparse])\n",
    "\n",
    "print(f\"ğŸ”¢ Ø´Ú©Ù„ Ù†Ù‡Ø§ÛŒÛŒ ÙÛŒÚ†Ø±Ù‡Ø§: {X_all.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08713630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Prepare Labels ------------------\n",
    "grouped = df.groupby(\"SampleName\")[\"MasterTestCode\"].apply(set).reset_index()\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_all = mlb.fit_transform(grouped[\"MasterTestCode\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399f429e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Ú¯Ø±ÙˆÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ ------------------\n",
    "grouped = df.groupby(\"SampleName\")[\"MasterTestCode\"].apply(set).reset_index()\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_all = mlb.fit_transform(grouped[\"MasterTestCode\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5d3313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Ø§Ù†ØªØ®Ø§Ø¨ SampleNameâ€ŒÙ‡Ø§ÛŒ ÛŒÚ©ØªØ§ Ùˆ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒ ------------------\n",
    "df_grouped = df.drop_duplicates(subset=\"SampleName\").set_index(\"SampleName\").loc[grouped[\"SampleName\"]].reset_index()\n",
    "\n",
    "# TF-IDF Ø§Ø² SampleName\n",
    "vectorizer = TfidfVectorizer(max_features=100)\n",
    "X_text = vectorizer.fit_transform(df_grouped[\"SampleName\"].astype(str))\n",
    "\n",
    "# ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ø§Ø² high_dependency_tests Ø¨Ø§ ÙˆØ²Ù†â€ŒØ¯Ù‡ÛŒ Ø²Ù…Ø§Ù†ÛŒ\n",
    "X_numeric = sample_features[[\n",
    "    \"FeBase_weighted\", \"Destruct_weighted\", \"IsLarge_weighted\",\n",
    "    \"StrongDepCount_weighted\",\n",
    "    \"FeBase_mean\", \"Destruct_mean\", \"IsLarge_mean\",\n",
    "    \"LogTotalCount\"\n",
    "]].fillna(0).values\n",
    "\n",
    "X_all = np.hstack([X_text.toarray(), X_numeric.to_numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38c51e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ ØªÙ‚Ø³ÛŒÙ…â€ŒØ¨Ù†Ø¯ÛŒ ------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e1541d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ø¨Ù‡ Ø§Ø²Ø§ÛŒ Ù‡Ø± Ù„ÛŒØ¨Ù„ ------------------\n",
    "models = []\n",
    "print(\"ğŸ¯ Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ LightGBM Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø¨Ø±Ú†Ø³Ø¨...\")\n",
    "for i in tqdm(range(y_train.shape[1]), desc=\"Training labels\"):\n",
    "    y_label = y_train[:, i]\n",
    "    model = LGBMClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    model.fit(X_train, y_label)\n",
    "    models.append(model)\n",
    "    \n",
    "# ------------------ Save Artifacts ------------------\n",
    "os.makedirs(\"model_output_new\", exist_ok=True)\n",
    "joblib.dump(models, \"model_output_new/lightgbm_models.pkl\")\n",
    "joblib.dump(vectorizer, \"model_output_new/vectorizer.pkl\")\n",
    "joblib.dump(mlb, \"model_output_new/label_binarizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e781423a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Save Artifacts ------------------\n",
    "os.makedirs(\"model_output\", exist_ok=True)\n",
    "joblib.dump(models, \"model_output/lightgbm_models.pkl\")\n",
    "joblib.dump(vectorizer, \"model_output/vectorizer.pkl\")\n",
    "joblib.dump(mlb, \"model_output/label_binarizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13aa6d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data_Final.csv\")\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Ø­Ø°Ù Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ø¨Ø¯ÙˆÙ† Ù†Ø§Ù… Ù†Ù…ÙˆÙ†Ù‡ ÛŒØ§ Ú©Ø¯ Ø¢Ø²Ù…ÙˆÙ†\n",
    "df = df.dropna(subset=[\"SampleName\", \"MasterTestCode\"])\n",
    "\n",
    "# Ø¬Ø¯Ø§ Ú©Ø±Ø¯Ù† Û±Û°Ùª Ø¨Ù‡â€ŒØµÙˆØ±Øª ØªØµØ§Ø¯ÙÛŒ\n",
    "df_sampled = df.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# Ø°Ø®ÛŒØ±Ù‡ ÙØ§ÛŒÙ„ ØªØ³Øª Ø¬Ø¯Ø§ Ø´Ø¯Ù‡\n",
    "df_sampled.to_csv(\"test_subset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df49b816",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, hamming_loss, label_ranking_average_precision_score\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "\n",
    "# ------------------ Load Models & Tools ------------------\n",
    "print(\"ğŸ“¦ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ùˆ Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§...\")\n",
    "models = joblib.load(\"model_output/lightgbm_models.pkl\")\n",
    "vectorizer = joblib.load(\"model_output/vectorizer.pkl\")  # fitted on SampleName\n",
    "mlb = joblib.load(\"model_output/label_binarizer.pkl\")\n",
    "print(f\"âœ… {len(models)} Ù…Ø¯Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯.\")\n",
    "\n",
    "# ------------------ Load Test Data ------------------\n",
    "print(\"ğŸ“„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÛŒ Ø¬Ø¯ÛŒØ¯...\")\n",
    "df = pd.read_csv(\"test_subset.csv\")\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# ------------------ Feature Engineering ------------------\n",
    "print(\"ğŸ§  Ø³Ø§Ø®Øª ÙÛŒÚ†Ø± Ø¨Ø±Ø§ÛŒ Ù‡Ø± SampleName...\")\n",
    "\n",
    "for col in [\"FeBase\", \"Destruct\", \"IsLarge\"]:\n",
    "    df[col] = df[col].fillna(0).astype(int)\n",
    "\n",
    "strong_deps = pd.read_csv(\"strong_test_dependencies.csv\")\n",
    "strong_counts = strong_deps.groupby(\"Test1\").size().reset_index(name=\"StrongDepCount\")\n",
    "df = df.merge(strong_counts, left_on=\"MasterTestCode\", right_on=\"Test1\", how=\"left\")\n",
    "df[\"StrongDepCount\"] = df[\"StrongDepCount\"].fillna(0)\n",
    "\n",
    "avg_phys = df.groupby(\"MasterTestCode\")[[\"FeBase\", \"Destruct\", \"IsLarge\"]].mean().reset_index()\n",
    "df = df.merge(avg_phys, on=\"MasterTestCode\", suffixes=(\"\", \"_mean\"))\n",
    "\n",
    "df_grouped = df.drop_duplicates(subset=\"SampleName\").set_index(\"SampleName\")\n",
    "#sample_names = sorted(set(df_grouped.index) & set(mlb.classes_))\n",
    "#df_grouped = df_grouped.loc[sample_names]\n",
    "\n",
    "# ------------------ Ø³Ø§Ø®Øª ÙÛŒÚ†Ø± Ù†Ù‡Ø§ÛŒÛŒ ------------------\n",
    "X_text = vectorizer.transform(df_grouped.index.astype(str))\n",
    "X_numeric = df_grouped[[\"FeBase\", \"Destruct\", \"IsLarge\", \"StrongDepCount\",\n",
    "                        \"FeBase_mean\", \"Destruct_mean\", \"IsLarge_mean\"]].fillna(0).values\n",
    "X_numeric_sparse = csr_matrix(X_numeric)\n",
    "X_new = hstack([X_text, X_numeric_sparse])\n",
    "print(f\"ğŸ”¢ Ø´Ú©Ù„ Ù†Ù‡Ø§ÛŒÛŒ ÙÛŒÚ†Ø±Ù‡Ø§: {X_new.shape}\")\n",
    "\n",
    "# ------------------ Predict ------------------\n",
    "print(\"ğŸ” Ø´Ø±ÙˆØ¹ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ...\")\n",
    "y_pred_proba = np.zeros((X_new.shape[0], len(models)))\n",
    "for i, model in tqdm(enumerate(models), total=len(models), desc=\"Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§\"):\n",
    "    try:\n",
    "        y_pred_proba[:, i] = model.predict_proba(X_new)[:, 1]\n",
    "    except ValueError as e:\n",
    "        print(f\"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù…Ø¯Ù„ {i}: {e}\")\n",
    "        continue\n",
    "\n",
    "# ------------------ Thresholding ------------------\n",
    "threshold = 0.75\n",
    "y_pred_bin = (y_pred_proba > threshold).astype(int)\n",
    "predicted_labels = mlb.inverse_transform(y_pred_bin)\n",
    "\n",
    "df_output = pd.DataFrame({\n",
    "    \"SampleName\": df_grouped.index,\n",
    "    \"Predicted_Tests\": predicted_labels\n",
    "})\n",
    "df_output.to_csv(\"predictions.csv\", index=False)\n",
    "print(\"âœ… Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§ Ø¯Ø± predictions.csv Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.\")\n",
    "\n",
    "# ------------------ Evaluation (Ø§Ú¯Ø± ground truth Ø¯Ø§Ø±ÛŒ) ------------------\n",
    "if \"MasterTestCode\" in df.columns:\n",
    "    print(\"ğŸ“Š Ø´Ø±ÙˆØ¹ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„...\")\n",
    "    label_series = df.groupby(\"SampleName\")[\"MasterTestCode\"].apply(lambda codes: list(set(codes)))\n",
    "    y_true = mlb.transform(label_series.reindex(df_grouped.index).fillna(\"\").apply(lambda x: [] if x == \"\" else x))\n",
    "\n",
    "    print(f\"ğŸ” LRAP: {label_ranking_average_precision_score(y_true, y_pred_proba):.4f}\")\n",
    "    print(f\"ğŸ¯ Micro F1: {f1_score(y_true, y_pred_bin, average='micro'):.4f}\")\n",
    "    print(f\"ğŸ¯ Micro Precision: {precision_score(y_true, y_pred_bin, average='micro'):.4f}\")\n",
    "    print(f\"ğŸ¯ Micro Recall: {recall_score(y_true, y_pred_bin, average='micro'):.4f}\")\n",
    "    print(f\"ğŸ¯ Hamming Loss: {hamming_loss(y_true, y_pred_bin):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929619dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "# Load label binarizer\n",
    "#mlb = joblib.load(\"model_output/label_binarizer.pkl\")\n",
    "\n",
    "# Calculate per-label metrics\n",
    "precisions = precision_score(y_true, y_pred_bin, average=None, zero_division=0)\n",
    "recalls = recall_score(y_true, y_pred_bin, average=None, zero_division=0)\n",
    "f1s = f1_score(y_true, y_pred_bin, average=None, zero_division=0)\n",
    "\n",
    "label_metrics = pd.DataFrame({\n",
    "    \"Label\": mlb.classes_,\n",
    "    \"Precision\": precisions,\n",
    "    \"Recall\": recalls,\n",
    "    \"F1 Score\": f1s\n",
    "})\n",
    "\n",
    "label_metrics_sorted = label_metrics.sort_values(\"F1 Score\", ascending=False).reset_index(drop=True)\n",
    "top_labels = label_metrics_sorted.head(10)\n",
    "bottom_labels = label_metrics_sorted.tail(10)\n",
    "\n",
    "def plot_label_metrics(df, title):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.barh(df[\"Label\"], df[\"F1 Score\"])\n",
    "    plt.xlabel(\"F1 Score\")\n",
    "    plt.title(title)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_label_metrics(top_labels, \"ğŸ¯ Ø¨Ù‡ØªØ±ÛŒÙ† 10 Ø¨Ø±Ú†Ø³Ø¨ Ø§Ø² Ù†Ø¸Ø± F1\")\n",
    "plot_label_metrics(bottom_labels, \"âš ï¸ Ø¶Ø¹ÛŒÙâ€ŒØªØ±ÛŒÙ† 10 Ø¨Ø±Ú†Ø³Ø¨ Ø§Ø² Ù†Ø¸Ø± F1\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (RasfAi)",
   "language": "python",
   "name": "new_rasfai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
