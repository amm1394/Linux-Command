{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e0b510e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from lightgbm import LGBMClassifier\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from lightgbm import LGBMClassifier, early_stopping\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, hamming_loss, label_ranking_average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "de907c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Load Base Data ------------------\n",
    "df = pd.read_csv(\"Data_Final.csv\")\n",
    "df.columns = df.columns.str.strip()\n",
    "df = df.dropna(subset=[\"SampleName\", \"MasterTestCode\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d2890e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Feature Engineering: Final with Time & Count Weights + SampleName Text ------------------\n",
    "# Ensure binary columns\n",
    "for col in [\"FeBase\", \"Destruct\", \"IsLarge\"]:\n",
    "    df[col] = df[col].fillna(0).astype(int)\n",
    "\n",
    "# Load dependency info\n",
    "strong_deps = pd.read_csv(\"strong_test_dependencies.csv\")\n",
    "strong_counts = strong_deps.groupby(\"Test1\").size().reset_index(name=\"StrongDepCount\")\n",
    "df = df.merge(strong_counts, left_on=\"MasterTestCode\", right_on=\"Test1\", how=\"left\")\n",
    "df[\"StrongDepCount\"] = df[\"StrongDepCount\"].fillna(0)\n",
    "\n",
    "# Mean physical attributes by MasterTestCode\n",
    "avg_phys = df.groupby(\"MasterTestCode\")[[\"FeBase\", \"Destruct\", \"IsLarge\"]].mean().reset_index()\n",
    "df = df.merge(avg_phys, on=\"MasterTestCode\", suffixes=(\"\", \"_mean\"))\n",
    "\n",
    "# --- Time-based Features ---\n",
    "df[\"MaxDate\"] = pd.to_datetime(df[\"MaxDate\"], errors=\"coerce\")\n",
    "now = pd.Timestamp.now()\n",
    "df[\"TestAgeDays\"] = (now - df[\"MaxDate\"]).dt.days.clip(lower=1)\n",
    "df[\"TimeWeight\"] = 1 / df[\"TestAgeDays\"]\n",
    "\n",
    "# --- Count-based Features ---\n",
    "df[\"TestImportance\"] = df[\"TestCount\"] * df[\"TimeWeight\"]\n",
    "df[\"LogTotalCount\"] = np.log1p(df[\"TotalCount\"])\n",
    "\n",
    "# --- Weighted Feature Columns ---\n",
    "df[\"WF_FeBase\"] = df[\"FeBase\"] * df[\"TestImportance\"]\n",
    "df[\"WF_Destruct\"] = df[\"Destruct\"] * df[\"TestImportance\"]\n",
    "df[\"WF_IsLarge\"] = df[\"IsLarge\"] * df[\"TestImportance\"]\n",
    "df[\"WF_StrongDep\"] = df[\"StrongDepCount\"] * df[\"TestImportance\"]\n",
    "\n",
    "# --- Group by SampleName ---\n",
    "sample_features = df.groupby(\"SampleName\").agg({\n",
    "    \"MasterTestCode\": lambda x: \" \".join(str(i) for i in x.dropna()),\n",
    "    \"WF_FeBase\": \"mean\",\n",
    "    \"WF_Destruct\": \"mean\",\n",
    "    \"WF_IsLarge\": \"mean\",\n",
    "    \"WF_StrongDep\": \"mean\",\n",
    "    \"FeBase_mean\": \"mean\",\n",
    "    \"Destruct_mean\": \"mean\",\n",
    "    \"IsLarge_mean\": \"mean\",\n",
    "    \"LogTotalCount\": \"mean\"\n",
    "})\n",
    "\n",
    "sample_features.rename(columns={\n",
    "    \"WF_FeBase\": \"FeBase_weighted\",\n",
    "    \"WF_Destruct\": \"Destruct_weighted\",\n",
    "    \"WF_IsLarge\": \"IsLarge_weighted\",\n",
    "    \"WF_StrongDep\": \"StrongDepCount_weighted\"\n",
    "}, inplace=True)\n",
    "\n",
    "# --- TF-IDF of SampleName (for similarity like Ù¾ÛŒÚ† m24 vs m42) ---\n",
    "sample_features[\"SampleName\"] = sample_features.index.astype(str)\n",
    "sample_vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1, 2))\n",
    "X_sample_name_text = sample_vectorizer.fit_transform(sample_features[\"SampleName\"])\n",
    "\n",
    "print(\"âœ… ÙÛŒÚ†Ø±Ù‡Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø§ Ø²Ù…Ø§Ù†ØŒ ØªÚ©Ø±Ø§Ø± Ùˆ Ù…ØªÙ† SampleName Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯Ù†Ø¯.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79147127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Filter Rare Labels Before Training ------------------\n",
    "label_counts = df.groupby(\"MasterTestCode\")[\"SampleName\"].nunique()\n",
    "threshold = 20  # Ø¢Ø³ØªØ§Ù†Ù‡: Ú©Ù…ØªØ± Ø§Ø² 20 Ù†Ù…ÙˆÙ†Ù‡\n",
    "rare_labels = label_counts[label_counts < threshold].index\n",
    "\n",
    "print(f\"âœ… ØªØ¹Ø¯Ø§Ø¯ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ÛŒ Ù†Ø§Ø¯Ø± Ø´Ù†Ø§Ø³Ø§ÛŒÛŒâ€ŒØ´Ø¯Ù‡: {len(rare_labels)}\")\n",
    "print(f\"âœ… ØªØ¹Ø¯Ø§Ø¯ Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ù‚Ø¨Ù„ Ø§Ø² ÙÛŒÙ„ØªØ±ÛŒÙ†Ú¯: {len(df)}\")\n",
    "\n",
    "# ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\n",
    "df_filtered = df[~df[\"MasterTestCode\"].isin(rare_labels)].copy()\n",
    "\n",
    "print(f\"âœ… ØªØ¹Ø¯Ø§Ø¯ Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ø¨Ø§Ù‚ÛŒÙ…Ø§Ù†Ø¯Ù‡ Ù¾Ø³ Ø§Ø² ÙÛŒÙ„ØªØ±ÛŒÙ†Ú¯: {len(df_filtered)}\")\n",
    "print(f\"âœ… Ø¯Ø±ØµØ¯ Ú©Ø§Ù‡Ø´ Ø¯Ø§Ø¯Ù‡: {100 * (1 - len(df_filtered)/len(df)):.2f}%\")\n",
    "\n",
    "# Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ú©Ø±Ø¯Ù† Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø§ØµÙ„ÛŒ Ø¨Ø§ Ù†Ø³Ø®Ù‡ ÙÛŒÙ„ØªØ±Ø´Ø¯Ù‡\n",
    "df = df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6116265b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ø¨Ø±Ø±Ø³ÛŒ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¨Ø¹Ø¯ Ø§Ø² ÙÛŒÙ„ØªØ±ÛŒÙ†Ú¯ Ø¨Ø¯ÙˆÙ† Ø¨Ø±Ú†Ø³Ø¨ Ù…Ø§Ù†Ø¯Ù‡â€ŒØ§Ù†Ø¯\n",
    "empty_samples = df.groupby(\"SampleName\")[\"MasterTestCode\"].apply(lambda x: len(set(x))==0)\n",
    "empty_samples = empty_samples[empty_samples].index.tolist()\n",
    "if empty_samples:\n",
    "    print(f\"âš ï¸ Ù‡Ø´Ø¯Ø§Ø±: {len(empty_samples)} Ù†Ù…ÙˆÙ†Ù‡ Ø¨Ø¯ÙˆÙ† Ø¨Ø±Ú†Ø³Ø¨ Ù¾Ø³ Ø§Ø² ÙÛŒÙ„ØªØ±ÛŒÙ†Ú¯ Ø¨Ø§Ù‚ÛŒ Ù…Ø§Ù†Ø¯Ù‡ Ø§Ø³Øª.\")\n",
    "    # Ø­Ø°Ù Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø¯ÙˆÙ† Ø¨Ø±Ú†Ø³Ø¨\n",
    "    df = df[~df[\"SampleName\"].isin(empty_samples)]\n",
    "    print(f\"âœ… Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø¯ÙˆÙ† Ø¨Ø±Ú†Ø³Ø¨ Ø­Ø°Ù Ø´Ø¯Ù†Ø¯. ØªØ¹Ø¯Ø§Ø¯ Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a07c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Vectorize SampleName ------------------\n",
    "# Ø­Ø°Ù Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ Ùˆ ØªÙ†Ø¸ÛŒÙ… index Ø±ÙˆÛŒ SampleName\n",
    "df_unique = df.drop_duplicates(subset=\"SampleName\").set_index(\"SampleName\")\n",
    "\n",
    "# Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ù‡Ù…â€ŒØªØ±Ø§Ø²ÛŒ Ú©Ø§Ù…Ù„ Ø¨ÛŒÙ† df Ùˆ sample_features Ø±ÙˆÛŒ SampleName\n",
    "common_samples = df_unique.index.intersection(sample_features.index)\n",
    "\n",
    "# ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ù‡Ø± Ø¯Ùˆ Ø¬Ø¯ÙˆÙ„\n",
    "df_grouped = df_unique.loc[common_samples]\n",
    "sample_features = sample_features.loc[common_samples]\n",
    "\n",
    "# Ø³Ø§Ø®Øª ÙˆÛŒÚ˜Ú¯ÛŒ Ù…ØªÙ†ÛŒ Ø§Ø² Ù†Ø§Ù… Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ (SampleName) Ø¨Ø§ TF-IDF\n",
    "vectorizer = TfidfVectorizer(analyzer=\"word\", ngram_range=(1, 2), max_features=100)\n",
    "X_sample_name_text = vectorizer.fit_transform(df_grouped.index.astype(str))\n",
    "\n",
    "print(f\"âœ… X_sample_name_text Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯. Ø´Ú©Ù„: {X_sample_name_text.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29a2a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ù†Ù‡Ø§ÛŒÛŒ ------------------\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "X_numeric = sample_features[[ \n",
    "    \"FeBase_weighted\", \"Destruct_weighted\", \"IsLarge_weighted\",\n",
    "    \"StrongDepCount_weighted\",\n",
    "    \"FeBase_mean\", \"Destruct_mean\", \"IsLarge_mean\",\n",
    "    \"LogTotalCount\"\n",
    "]].fillna(0).values\n",
    "\n",
    "X_numeric_sparse = csr_matrix(X_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4340a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Prepare Labels ------------------\n",
    "grouped = df.groupby(\"SampleName\")[\"MasterTestCode\"].apply(set).reset_index()\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_all = mlb.fit_transform(grouped[\"MasterTestCode\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0568eabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ ØªØ±Ú©ÛŒØ¨ Ù†Ù‡Ø§ÛŒÛŒ Ù‡Ù…Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ ------------------\n",
    "X_all = hstack([X_sample_name_text, X_numeric_sparse])\n",
    "print(f\"ğŸ”¢ Ø´Ú©Ù„ Ù†Ù‡Ø§ÛŒÛŒ ÙÛŒÚ†Ø±Ù‡Ø§: {X_all.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3006e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ ØªÙ‚Ø³ÛŒÙ…â€ŒØ¨Ù†Ø¯ÛŒ ------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6f4322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Ø­Ø°Ù Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ù†Ø§Ø¯Ø± Ø¨Ø§ positiveÙ‡Ø§ÛŒ Ø®ÛŒÙ„ÛŒ Ú©Ù… ------------------\n",
    "MIN_POS_COUNT = 60\n",
    "\n",
    "# ------------------ Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªØ¹Ø¯Ø§Ø¯ positive Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú©Ù„Ø§Ø³ ------------------\n",
    "label_pos_counts = y_all.sum(axis=0)\n",
    "all_classes = np.array(mlb.classes_)\n",
    "\n",
    "# Ø­Ø°Ù Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ ØªØ¹Ø¯Ø§Ø¯ Ù…Ø«Ø¨Øªâ€ŒÙ‡Ø§Ø´ÙˆÙ† Ø®ÛŒÙ„ÛŒ Ú©Ù…Ù‡\n",
    "MIN_POS_COUNT = 80\n",
    "valid_mask = label_pos_counts >= MIN_POS_COUNT\n",
    "print(f\"âœ… ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ù…Ø¹ØªØ¨Ø± Ù¾Ø³ Ø§Ø² ÙÛŒÙ„ØªØ±: {valid_mask.sum()} Ø§Ø² {y_all.shape[1]}\")\n",
    "\n",
    "# ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ø¨Ø± Ø§Ø³Ø§Ø³ mask\n",
    "y_all_filtered = y_all[:, valid_mask]\n",
    "mlb_filtered = deepcopy(mlb)\n",
    "mlb_filtered.classes_ = all_classes[valid_mask]\n",
    "\n",
    "# ------------------ ØªÙ†Ø¸ÛŒÙ… min_data_in_leaf ------------------\n",
    "avg_positives_per_class = np.mean(label_pos_counts[valid_mask])\n",
    "min_data_leaf_value = max(10, int(avg_positives_per_class * 0.05))\n",
    "print(f\"ğŸ”§ Ù…Ù‚Ø¯Ø§Ø± ØªÙ†Ø¸ÛŒÙ…â€ŒØ´Ø¯Ù‡ min_data_in_leaf: {min_data_leaf_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5b934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§ ------------------\n",
    "models = []\n",
    "print(\"ğŸš€ Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§...\")\n",
    "\n",
    "for i in tqdm(range(y_all_filtered.shape[1]), desc=\"Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§\", unit=\"Ú©Ù„Ø§Ø³\"):\n",
    "    X_train, X_val, y_train_i, y_val_i = train_test_split(\n",
    "        X_all, y_all_filtered[:, i], test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    model = LGBMClassifier(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.05,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        is_unbalance=True,\n",
    "        min_data_in_leaf=min_data_leaf_value\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train_i,\n",
    "        eval_set=[(X_val, y_val_i)],\n",
    "        eval_metric=\"binary_logloss\",\n",
    "        callbacks=[early_stopping(stopping_rounds=20)],\n",
    "    )\n",
    "\n",
    "    models.append(model)\n",
    "\n",
    "# ------------------ Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ùˆ Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ ------------------\n",
    "os.makedirs(\"model_output_new\", exist_ok=True)\n",
    "joblib.dump(models, \"model_output_new/lightgbm_models.pkl\")\n",
    "joblib.dump(vectorizer, \"model_output_new/vectorizer.pkl\")\n",
    "joblib.dump(mlb_filtered, \"model_output_new/label_binarizer.pkl\")\n",
    "\n",
    "print(\"âœ… Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ùˆ Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943d5bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ú©Ù„ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ (ÙÙ‚Ø· ÛŒÚ©â€ŒØ¨Ø§Ø±)\n",
    "X_train_all, X_test_all, y_train_all, y_test_all = train_test_split(\n",
    "    X_all, y_all_filtered, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù…Ø¯Ù„ Ø±ÙˆÛŒ X_test_all\n",
    "y_pred = np.zeros_like(y_test_all)\n",
    "y_scores = np.zeros_like(y_test_all, dtype=float)\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    y_pred[:, i] = model.predict(X_test_all)\n",
    "    try:\n",
    "        y_scores[:, i] = model.predict_proba(X_test_all)[:, 1]\n",
    "    except:\n",
    "        y_scores[:, i] = 0.0  # Ø§Ú¯Ø± predict_proba Ù†Ø¯Ù‡ (Ù…Ø«Ù„Ø§Ù‹ Ù…Ø¯Ù„ Ø®Ø±Ø§Ø¨ Ø¨Ø§Ø´Ù‡)\n",
    "\n",
    "# Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§\n",
    "micro_f1 = f1_score(y_test_all, y_pred, average='micro')\n",
    "micro_precision = precision_score(y_test_all, y_pred, average='micro')\n",
    "micro_recall = recall_score(y_test_all, y_pred, average='micro')\n",
    "lrap_score = label_ranking_average_precision_score(y_test_all, y_scores)\n",
    "hamming = hamming_loss(y_test_all, y_pred)\n",
    "\n",
    "# Ø®Ø±ÙˆØ¬ÛŒ Ù†Ù‡Ø§ÛŒÛŒ\n",
    "results = {\n",
    "    \"Micro F1\": micro_f1,\n",
    "    \"Micro Precision\": micro_precision,\n",
    "    \"Micro Recall\": micro_recall,\n",
    "    \"LRAP\": lrap_score,\n",
    "    \"Hamming Loss\": hamming\n",
    "}\n",
    "\n",
    "print(\"âœ… Ù†ØªØ§ÛŒØ¬ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„:\")\n",
    "for k, v in results.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (RasfAi)",
   "language": "python",
   "name": "new_rasfai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
